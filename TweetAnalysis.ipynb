{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49429585/how-to-change-the-background-color-of-a-single-cell-in-a-jupyter-notebook-jupy\n",
    "from IPython.core.magic import register_line_magic\n",
    "from IPython.display import HTML, display\n",
    "import json\n",
    "\n",
    "@register_line_magic\n",
    "def bg(color, cell=None):    \n",
    "    script = (\n",
    "        \"var n = [this.closest('.cell,.jp-CodeCell')];\"\n",
    "        \"n = n.concat([].slice.call(n[0].querySelectorAll('.input_area,.highlight,.jp-Editor')));\"\n",
    "        f\"n.forEach(e=>e.style.background='{color}');\"\n",
    "        \"this.parentNode.removeChild(this)\"\n",
    "    )\n",
    "    display(HTML(f'<img src onerror=\"{script}\" style=\"display:none\">'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6dde3",
   "metadata": {},
   "source": [
    "# **Integrated CA**:  \n",
    "## Twitter API Data Analysis\n",
    "\n",
    "*Lecturers:* Muhammad Iqbal, David McQuaid\n",
    "\n",
    "*Student:* [2020491]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ee95a",
   "metadata": {},
   "source": [
    "## **Data Understanding:**\n",
    "\n",
    "### **Data Collection:**\n",
    "#### **Data Source:**\n",
    "**`Twitter API`**<br>\n",
    "The Twitter API enables programmatic access to Twitter elements such as: Tweets, Direct Messages, Spaces, Lists, users etc.<br>\n",
    "\n",
    "https://developer.twitter.com/en/docs/twitter-api\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### **Datasets:** \n",
    "**`ProjectTweets`**<br>\n",
    "Created using Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370e985",
   "metadata": {},
   "source": [
    "#### Import Modules\n",
    "The required packages are imported for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install plotly#==5.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c284c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    import statistics as st # Statistical Module(in Standard Library)\n",
    "    import researchpy as rp\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "except ModuleNotFoundError:\n",
    "    !pip install seaborn numpy matplotlib researchpy plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01abe183",
   "metadata": {},
   "source": [
    "#### Loading the Datasets\n",
    "Takes a couple of seconds to load in the dataset. Huge improvement from the minutes it took before initial cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"./Data/ProjectTweets.csv\")#,encoding = 'utf-16-le',sep='\\t')#latin'utf-16',on_bad_lines='skip')#,lineterminator='\\n')#,encoding_errors='replace')#,error_bad_lines=False)#)encoding=\"windows-1252\"\n",
    "#df_iso = pd.read_excel(\"./Data/_ISO-3166.xlsx\")#sheet_name=0, header=0, names=None, index_col=None, usecols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd87ca1",
   "metadata": {},
   "source": [
    "#### **`Data Naming`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bda89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning row names to data without headers\n",
    "cols = [\"ids\",\"date\",\"flag\",\"user\",\"text\"]\n",
    "df = pd.read_csv('./Data/ProjectTweets.csv', header=None, names=cols)#, index_col=\"ids\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32752e27",
   "metadata": {},
   "source": [
    "### **Data Description:**\n",
    "To analyse the data, I conducted initial analysis on the dataset that can be seen step-by-step below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb92de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e1065",
   "metadata": {},
   "source": [
    "#### Data Size & Dimensions: \n",
    "The data size of **1600000** entry meets the requirements of accurate model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b997fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "print(\"Row:{} x Columns:{}\".format(df.shape[0],df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e19ec",
   "metadata": {},
   "source": [
    "#### Data Overview\n",
    "The `head` and `tail` functions show the first and last N observations in the data set which gives a glimpse at the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "display(df.head(N))\n",
    "display(df.tail(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f026312",
   "metadata": {},
   "source": [
    "The `info` function depicts data about each variable in the data set as well as the dataset as a whole.\n",
    "There are 3 columns and 1600000 rows. There are no missing values in the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62852d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose:full_summary   null_counts:show_NaN   memory_usage:Exact_Size  \n",
    "df.info(verbose=True, null_counts=True,memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f59c9",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e2b27",
   "metadata": {},
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64e963",
   "metadata": {},
   "source": [
    "Error arises when trying to make **ids** the id of the dataframe.\n",
    "So verification of possible duplication is required!\n",
    "#### IDs Duplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a244134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 1685 non-uniquely identified texts!!\n",
    "print(len(df.ids)-len(df.ids.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec864cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset='ids', keep=False)].sort_values(by='ids', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b67efa",
   "metadata": {},
   "source": [
    "It seems that the queries that have identical `ids` also have all their data duplicated. But still need to be verified.\n",
    "#### Record Duplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicate rows with all columns the same\n",
    "duplicate_rows = df[df.duplicated(keep=False)]\n",
    "\n",
    "if duplicate_rows.empty:\n",
    "    print(\"No duplicate rows with all columns the same!\")\n",
    "else:\n",
    "    print(\"There are duplicate rows with all columns the same:\")\n",
    "    print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66836040",
   "metadata": {},
   "source": [
    "The records with the same ids are duplicate records so these data's are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='last', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e71cb",
   "metadata": {},
   "source": [
    "#### Missing Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b5225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b387f",
   "metadata": {},
   "source": [
    "#### **Irrelevant Data**\n",
    "As seen below, the only value the `flag` variable has is NO_QUERY hence it can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.flag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('flag', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932871dc",
   "metadata": {},
   "source": [
    "The `ids` column is unnecessary for the analysis, since the best way to sort this data is with the date of the tweet and then indexing it accordingly without the need to store and handle large integer values (i.e. Big integers). \n",
    "\n",
    "Not to mention the different issues that may arise from big integers such as:\n",
    "- **Memory Usage** which limits the capacity of the analysis\n",
    "- **Performing arithmetic** operations slow down with increased bit size for calculations\n",
    "- **Computation** Libraries/Functions might not be optimized for big integers, leading to slower computations or potential errors\n",
    "- **Overflow** operations on big integers may cause overflow errors if arbitrary precision arithmetic not handled\n",
    "- **Visualization** Big integers can cause issues with data visualization libraries, e.g. when generating charts & graphs. Also, Scaling Big integer axes might also lead to distorted visualizations.\n",
    "\n",
    "For these reasons the `ids` column can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('ids', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69bfef",
   "metadata": {},
   "source": [
    "#### Text Formatting\n",
    "Text needs to be converted from object to string format to be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'text' column to string\n",
    "df['text'] = df['text'].astype(str)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac5c17",
   "metadata": {},
   "source": [
    "#### Date Formatting\n",
    "\n",
    "To be able to sort the data based on date, the date needs to be converted to `datetime format`. For this the  date string format needs to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample e.g. 3rd indexed date\n",
    "df.iloc[2:3].date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7838084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Format\n",
    "#date_format = \"%a %b %d %H:%M:%S %Z %Y\"\n",
    "\n",
    "#df['date'].apply(lambda x: datetime.strptime(x, date_format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21690c",
   "metadata": {},
   "source": [
    "The Format of the date is:\n",
    "- %a: Weekday abbreviated (e.g. Mon)\n",
    "- %b: Month abbreviated (e.g Apr)\n",
    "- %d: Day of the month as a zero padded number (e.g. 06)\n",
    "- %H: Hour (24-hour) as a zero padded number (e.g. 22)\n",
    "- %M: Minute as a zero padded number (e.g. 19)\n",
    "- %S: Second as a zero padded number (e.g. 53)\n",
    "- %Z: Time zone (e.g. PDT)\n",
    "- %Y: Year (e.g. 2009)\n",
    "\n",
    "When trying to use `datetime` library ValueError occurs due to the *PDT* and *PST* time zone abbreviations,  not being directly supported by the strptime() function. To handle such time zone abbreviations, I use the pytz library along with dateutil for parsing the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51403746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytz python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e365fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datetime import datetime\n",
    "import pytz\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee2f7e",
   "metadata": {},
   "source": [
    "##### `Setting Timezone to UTC`\n",
    "Using the *tz_convert* method to make sure the timezone is set to UTC to avoid timezone related issues (possibly in e.g. plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a couple of minutes\n",
    "df['date'] = df['date'].apply(lambda x: parser.parse(x).astimezone(pytz.utc))\n",
    "df['date'] = df['date'].dt.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cf692",
   "metadata": {},
   "source": [
    "#### Data Sorting with Date\n",
    "Reindexing the data by sorting based on date from oldest to most recent tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='date').reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f50f19",
   "metadata": {},
   "source": [
    "#### Store Processed Data\n",
    "To save time with the analysis processed data is saved here to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da07665",
   "metadata": {},
   "source": [
    "### Possible Scam & Irrelevant Data\n",
    "Any tweet has the probability to contain malicious link to check its validity would take extensive analysis not to mention how it is an irrelevant data for the analysis to be conducted. For the these reasons it is removed for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular Expression Pattern (to find websites)\n",
    "pattern = r'\\b(?:https?://|www\\.)\\S+(?=\\s|$)|\\S+\\.com\\b'  # Match start: http,https,www. & end with'.com',before whitespace  \n",
    "\n",
    "# Website Extraction from each row in the 'text' column\n",
    "websites_list = df['text'].apply(lambda x: re.findall(pattern, x))\n",
    "\n",
    "# List Flattening (removing null values & merge into one list)\n",
    "websites = [website for sublist in websites_list for website in sublist]\n",
    "\n",
    "print(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe51ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "websites_counts = Counter(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88289c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs from 'text' column's each row\n",
    "df.text = df.text.apply(lambda x: re.sub(pattern, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checker\n",
    "#[website for sublist in df['text'].apply(lambda x: re.findall(pattern, x)) for website in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba962753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcebd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of a word in the 'text' for each date (only showing with result)\n",
    "df['text'][df['text'].apply(lambda x: x.lower().count('school') > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba2316",
   "metadata": {},
   "source": [
    "### Duplicate Texts\n",
    "Duplicate texts over a period of time may arise for a number of reasons such as:\n",
    "- After cleaning the text from URLs it is possible that some might have become empty (i.e. text only had link). These need to be removed as they contain no information about the sentiment of the tweets. They are useful for initial analysis to check the users presence frequency.\n",
    "- Users might follow a trend or have a strong emotion to certain topics, these can be left in to emphasis their sentimental impact on the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset='text', keep=False)].sort_values(by='text', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading & trailing whitespaces from 'text' (to show it as empty)\n",
    "df.text = df.text.str.strip()\n",
    "\n",
    "# Removing rows with empty 'text'\n",
    "df = df[df['text'].notna() & (df['text'] != '')]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5568c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset='text', keep=False)].sort_values(by='text', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3cad0",
   "metadata": {},
   "source": [
    "### HTML Character Entities\n",
    "Having Entity names could affect the analysis for a number of reasons including:\n",
    "- *Interpretation error* e.g. sentiment analysis may have the meaning of the text distorted\n",
    "- *Tokenization Disruption* with possible incorrect word counts\n",
    "- *ML* introducing additional features not contributing to text meaning could impact the *model*'s performance & generalization\n",
    "- *Data Cleaning* HTML entities can be considered noise during text analysis\n",
    "- ...\n",
    "\n",
    "To avoid it these are removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace HTML entities in the 'text' column\n",
    "df['text'] = df['text'].str.replace('&gt;', '>', regex=True)\n",
    "df['text'] = df['text'].str.replace('&lt;', '<', regex=True)\n",
    "df['text'] = df['text'].str.replace('&amp;', '&', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4897afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML entities in the 'text' column\n",
    "df['text'] = df['text'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d3141",
   "metadata": {},
   "source": [
    "### Possible Inappropriate Texts\n",
    "Throughout the preprocessing phase of the analysis I have found possible culprits of inappropriate texting behavior. Although this would need further analysis later on since talking about this topic does not necesarily imply bad faith but filtering this out is time consumming. Furthermore, it could shed light to a possible regular cyber attacks by certain menacing individuals. Additionally, even if an uncomfortable topic it can still hold sentimental value so might be left in for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb9829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\bNSFW\\b|\\bsex\\b'\n",
    "\n",
    "df[df['text'].str.contains(pattern, case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1095400, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843074a",
   "metadata": {},
   "source": [
    "# TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b685f",
   "metadata": {},
   "source": [
    "### Dataset's Timespan\n",
    "Calculating the span by subtracting the earliest from the latest date.\n",
    "Below, it shows the dataset contains 79 days, 12 hours, 8 minutes and 46 seconds of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c26b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timespan of the Dataset\n",
    "df['date'].max() - df['date'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b4cf8",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "Checking the keywords used by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ea39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'#(\\w+)'\n",
    "hashtags_list = [mention for sublist in df['text'].apply(lambda x: re.findall(pattern, x)) for mention in sublist]\n",
    "#mentions = [mention for sublist in mentions_list for mention in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ed9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "hashtags_counts = Counter(hashtags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3aed2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sorted(hashtags_counts)\n",
    "\n",
    "# Descending Order\n",
    "hashtags_counts.most_common()\n",
    "\n",
    "# Ascending Order\n",
    "#sorted(hashtags_counts.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f759588",
   "metadata": {},
   "source": [
    "#### `Wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# WordCloud object Creation\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(hashtags_counts)\n",
    "\n",
    "# Plot Creation\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Top Hashtags used by Users', size=20)\n",
    "\n",
    "# Plot Showing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84b8f7",
   "metadata": {},
   "source": [
    "### Popular Users\n",
    "Checking the users popularly tweeted by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'@(\\w+)'\n",
    "user_list = [mention for sublist in df['text'].apply(lambda x: re.findall(pattern, x)) for mention in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "user_counts = Counter(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(user_counts)\n",
    "\n",
    "# Descending Order\n",
    "user_counts.most_common()\n",
    "\n",
    "# Ascending Order\n",
    "#sorted(user_counts.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58879ced",
   "metadata": {},
   "source": [
    "#### `Wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960eca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# WordCloud object Creation\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(user_counts)\n",
    "\n",
    "# Plot Creation\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Popular Users tweeted by Users', size=20)\n",
    "\n",
    "# Plot Showing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838985e1",
   "metadata": {},
   "source": [
    "### 20 Top Tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Top Tweeters\n",
    "df['user'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Top Tweeters over the span of dataset\n",
    "tweeters = df['user'].value_counts().head(20)\n",
    "\n",
    "# Seaborn Style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Bar plot using Seaborn\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x=tweeters.index, y=tweeters.values, color='darkgreen')\n",
    "plt.title('20 Top Tweeters',fontsize=30, color='green', backgroundcolor='honeydew')\n",
    "plt.xlabel('Tweeters')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "ax.set_xticklabels([item for item in tweeters.index], rotation=45)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5a562",
   "metadata": {},
   "source": [
    "### 20 Least Tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ba8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Least Tweeters\n",
    "df['user'].value_counts().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d04a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Least Tweeters over the span of dataset\n",
    "tweeters = df['user'].value_counts().tail(20)\n",
    "\n",
    "# Seaborn Style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Bar plot using Seaborn\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x=tweeters.index, y=tweeters.values, color='maroon')\n",
    "plt.title('20 Least Tweeters',fontsize=30, color='red', backgroundcolor='lavenderblush')\n",
    "plt.xlabel('Tweeters')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "ax.set_xticklabels([item for item in tweeters.index], rotation=45)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf257723",
   "metadata": {},
   "source": [
    "### Average Number of Tweets per Tweeter (over a span of 79 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of Tweets per Tweeter (over a span of 79 days)\n",
    "df['user'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0f914",
   "metadata": {},
   "source": [
    "### Average Number of tweets per day (over a span of 79 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of tweets per day\n",
    "df.groupby(df['date'].dt.date).size().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d831a2",
   "metadata": {},
   "source": [
    "### Total number of tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55d4b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total number of tweets per day\n",
    "df.groupby(df['date'].dt.date).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATEFORMATTER NOT WORKING --> from matplotlib.dates import DateFormatter\n",
    "# USING APPLY FUNCTION\n",
    "\n",
    "# Plot the number of tweets per day\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax = df.groupby(df['date'].dt.date).size().plot(kind='bar',\n",
    "                                                color='indigo',edgecolor='plum', alpha=0.5)\n",
    "plt.title('Tweet Distribution',color= \"goldenrod\", fontsize=22,weight='bold',fontname=\"Times New Roman\") \n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Tweets Per Day')\n",
    "\n",
    "# Format tick labels to show only month and day\n",
    "ax.set_xticklabels([item.strftime('%b %d') for item in df.groupby(df['date'].dt.date).size().index], rotation=45)\n",
    "\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "#date_form = DateFormatter(\"%d %b\")\n",
    "#ax.xaxis.set_major_formatter(date_form)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.suptitle(\"Tweet Distribution\", color= \"gold\", fontsize=22,weight='bold',fontname=\"Times New Roman\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59a58c",
   "metadata": {},
   "source": [
    "### Regular Users only\n",
    "If over the span of 79 days, a user hasn't at least tweeted 79 tweets then they are not considered regular i.e. equating to at least one tweet per day.\n",
    "Below it shows that out of a total of **659765** users only **322** are regular users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02504234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.user.value_counts()[df['user'].value_counts() >= 79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.user.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d71eb",
   "metadata": {},
   "source": [
    "### Users' Tweeting Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nunber of Tweets per day\")\n",
    "display(df.groupby([\"user\",df['date'].dt.month, df['date'].dt.day]).count())#.sort_values()#[\"Year\"],ascending=True)\n",
    "print(\"Tweets per day\")\n",
    "display(df.groupby([\"user\",df['date'].dt.month, df['date'].dt.day]).sum())#.sort_values()#[\"Year\"],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a62ef",
   "metadata": {},
   "source": [
    "### Users' Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Statistics\n",
    "df[\"user\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d348199",
   "metadata": {},
   "source": [
    "### Remove User names, Hashtags, Special Characters, Digits & Extra Spaces\n",
    "To find the sentiment of the users hashtags and user names do not contribute to further analysis. So they are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text column\n",
    "def clean_text(text):\n",
    "    # Removing Words starting with '#' or '@'\n",
    "    text = re.sub(r'[@#]\\w+\\s*', ' ', text)\n",
    "    \n",
    "    # Removing Special chars & Digits\n",
    "    #text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Removing Special chars & Digits except apostrophe (used for contraction handling!!!)\n",
    "    text = re.sub(r'[^a-zA-Z\\s\\']', '', text)\n",
    "    \n",
    "    # Lowercase Conversion\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing Extra Space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['text'].apply(clean_text).astype(str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0723be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ffc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP DATA!!!\n",
    "df2 = df.copy()\n",
    "#df = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea29dd",
   "metadata": {},
   "source": [
    "#### `Check Empty texts` after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb398cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading & trailing whitespaces from 'text' (to show it as empty)\n",
    "df.text = df.text.str.strip()\n",
    "\n",
    "# Removing rows with empty 'text'\n",
    "df = df[df['text'].notna() & (df['text'] != '')]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "df[df.duplicated(subset='text', keep=False)].sort_values(by='text', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96b03c",
   "metadata": {},
   "source": [
    "### Change Date Format\n",
    "For the analysis the time (hour,minute, second) the tweet was sent is irrelevant. Only the date is important hence the date is changed to only include the year, month and day of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca22d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating 'date' column to 'YYYY-MM-DD'\n",
    "#df['date'] = df['date'].dt.strftime('%Y-%m-%d') # CREATES ERROR column not recognized as datetime obj\n",
    "df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e908a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb28714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP DATA!!!\n",
    "#df2 = df.copy()\n",
    "#df = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527407e8",
   "metadata": {},
   "source": [
    "### Remove User Column\n",
    "For the overall sentiment analysis it is irrelevant which user tweeted, the overall sentiment of the data is  crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"user\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea9aef",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "Before any text analysis can be performed text processing needs to be done. Below are the techiques used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092c1bb",
   "metadata": {},
   "source": [
    "## Expanding Contractions\n",
    "\n",
    "For the efficiency of the next step, i.e. removing stopwords, it is crucial to expand the contractions to the groups of words that they represent. e.g. didn't to did not... Athough it is important to note that in the text some of these are missing apostrophe signs in the text so it needs to be handled too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a6751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f41be",
   "metadata": {},
   "source": [
    "Using language model spaCy to tokenize the text and leverage its lemmatization feature to correctly handle contractions and missing apostrophes e.g. **thats** to **that is**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading spaCy language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45dca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand contractions (including missing apostrophe)\n",
    "def expand_contractions(text):\n",
    "    # regex to add the missing apostrophes\n",
    "    #text = re.sub(r\"\\b(\\w+)(s|re)\\b\", r\"\\1 '\\2\", text)  \n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Takes a couple minutes!!!!\n",
    "# Apply contraction expansion function to the 'text' column\n",
    "df['preprocessed_text'] = df['cleaned_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP DATA!!!\n",
    "#df2 = df.copy()\n",
    "#df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24fb3f",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "Stopwords (e.g. the, and, is, of, in etc.) being commonly used words that generally don't carry meaning. I will remove them for a number of reasons:\n",
    "\n",
    "- reduce `noise` (in text analysis)\n",
    "- improve `efficiency` reducing word size to process speeding up algorithms and save memory\n",
    "- better `interpretation` for topic modeling more meaningful and interpretable topics\n",
    "    \n",
    "\n",
    "It is important to consider how sometimes stopwords might be crucial for instance: in sentiment analysis some stopwords like **not** can change the meaning. In such cases, retaining certain stopwords.\n",
    "\n",
    "Hence, I will retain stopwords 'no' and 'not' to presence the meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Defining stopwords with exceptions\n",
    "stop_words = set(stopwords.words('english')) - {'no', 'not'}\n",
    "\n",
    "def stopwords_removal_with_exception(text, exceptions):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words or word in exceptions]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: stopwords_removal_with_exception(x, {'no', 'not'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7192986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6825b2b",
   "metadata": {},
   "source": [
    "## Text Tokenization & Stemming/Lemmatization\n",
    "Using Porter stemming algorithm, to Stem the text has resulted in incorrect stemming such as his became hi, body became bodi. For this reason I decided to use lemmatization to increase the chances to produce a valid base form (i.e. lemma) of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Initialize stemmer\n",
    "    #stemmer = PorterStemmer()\n",
    "    # Stem words\n",
    "    #stemmed_words = [stemmer.stem(word) for word in words] \n",
    "    #return ' '.join(stemmed_words)\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    # Lemmatize words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]    \n",
    "    return ' '.join(lemmatized_words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a couple minutes!!!!\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP DATA!!!\n",
    "df2 = df.copy()\n",
    "#df = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d880c",
   "metadata": {},
   "source": [
    "# Sentiment Labeling\n",
    "Assigning sentiment labels to the text: positive, negative or neutral based on the sentiment expressed in the text. \n",
    "\n",
    "I will use the **VADER** sentiment analysis tool from the NLTK library, which is a pre-trained sentiment analysis models trained on large text datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de70efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon') # used by SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32437810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SentimentIntensityAnalyzer Initialisation\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Assigning Sentiment Labels\n",
    "def sentiment_label(text):\n",
    "    \n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    \n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    \n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment labeling function to the 'text' column\n",
    "df['sentiment'] = df['preprocessed_text'].apply(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0795c7a",
   "metadata": {},
   "source": [
    "# Store & Load dataframe for Modelling\n",
    "Since modeling is time consuming it is advisable to store the prepared data in order to avoid running through the code again and again when Kernel needs to be restarted or cleared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad3fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORE\n",
    "#PATH = './data/cleanedProjectTweets.csv'\n",
    "#df.to_csv(PATH, index=False)  # Set index=False avoids saving row numbers as column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574dfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD!!!\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/cleanedProjectTweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cda337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up datatypes\n",
    "\n",
    "# Formating 'date' column to 'YYYY-MM-DD'\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "#df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "df.drop('text', axis=1,inplace=True) #df['text'].astype('string')\n",
    "df.drop('cleaned_text', axis=1,inplace=True) #df['cleaned_text'].astype('string')\n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].astype('string')\n",
    "df['sentiment'] = df['sentiment'].astype('string')\n",
    "\n",
    "#df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e29e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1598285 entries, 0 to 1598284\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count    Dtype         \n",
      "---  ------             --------------    -----         \n",
      " 0   date               1598285 non-null  datetime64[ns]\n",
      " 1   preprocessed_text  1590126 non-null  string        \n",
      " 2   sentiment          1598285 non-null  string        \n",
      "dtypes: datetime64[ns](1), string(2)\n",
      "memory usage: 36.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb393486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='last', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6eda5",
   "metadata": {},
   "source": [
    "#### `Check Empty texts` after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading & trailing whitespaces from 'text' (to show it as empty)\n",
    "df.preprocessed_text = df.preprocessed_text.str.strip()\n",
    "\n",
    "# Removing rows with empty 'text'\n",
    "df = df[df['preprocessed_text'].notna() & (df['preprocessed_text'] != '')]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc22bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'TextColumn' has only one character or none\n",
    "df = df[~df['preprocessed_text'].apply(lambda x: len(str(x)) <= 1)]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c7d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['preprocessed_text'],keep='last', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547ac8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, preprocessed_text, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicates\n",
    "df[df.duplicated(subset='preprocessed_text', keep=False)].sort_values(by='preprocessed_text', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0c8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1469845 entries, 0 to 1469844\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count    Dtype         \n",
      "---  ------             --------------    -----         \n",
      " 0   date               1469845 non-null  datetime64[ns]\n",
      " 1   preprocessed_text  1469845 non-null  string        \n",
      " 2   sentiment          1469845 non-null  string        \n",
      "dtypes: datetime64[ns](1), string(2)\n",
      "memory usage: 33.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413e303",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "After Processing the text data, training the sentiment analysis model can begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ddd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7b7c4",
   "metadata": {},
   "source": [
    "## Smaller Data portion \n",
    "Using frac of the data to speed up experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9360f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller data portion faster experimentation\n",
    "df_sample = df.sample(frac=0.0001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0679de1",
   "metadata": {},
   "source": [
    "## Train & Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23004479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df_sample, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0eeace",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this phase, the text is converted into numerical features usable for training. I will use a common approach using the TF-IDF (Term Frequency-Inverse Document Frequency) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "952277d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using TF-IDF with fewer features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000) # adjustable feature number (lower->less accurate but faster)\n",
    "\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df['preprocessed_text'])\n",
    "y_train = train_df['sentiment']\n",
    "X_test = tfidf_vectorizer.transform(test_df['preprocessed_text'])\n",
    "y_test = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8dca116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers Defining\n",
    "classifiers = [\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('SVM', SVC()),\n",
    "    ('Random Forest', RandomForestClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b178fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Test Accuracy: 0.63\n",
      "SVM - Test Accuracy: 0.57\n",
      "Random Forest - Test Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Training & Evaluation each classifier\n",
    "for name, classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name} - Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b235d1",
   "metadata": {},
   "source": [
    "# ML Models & Algorithms Selection Rationale\n",
    "\n",
    "## **`Models Selection`**\n",
    "In sentiment analysis, the main goal is to classify text into different sentiment categories (i.e. positive, negative, neutral). The models selection is based on their ability to:\n",
    "- handle text data\n",
    "- capture patterns\n",
    "- generalize well to new, unseen data\n",
    "\n",
    "1. `Naive Bayes` simple & fast algorithm that works well with text data. It's based on Bayes' theorem and assumes that the features (i.e. words) are conditionally independent given the class label. Despite its \"naive\" assumption, it often performs well for text classification tasks.\n",
    "\n",
    "2. `Support Vector Machine (SVM)` effective for text classification when combined with appropriate feature representations like TF-IDF. They aim to find a hyperplane that maximally separates different classes. SVMs can handle high-dimensional data well and are known for their ability to find complex decision boundaries.\n",
    "\n",
    "3. `Random Forest` an ensemble method combining multiple decision trees to make predictions. It's robust, handles non-linear relationships, and can capture interactions between features. Random Forest can work well for text classification when combined with features like TF-IDF.\n",
    "\n",
    "## **`Feature Extraction`**\n",
    "\n",
    "`Term Frequency-Inverse Document Frequency (TF-IDF)` a common feature representation choice for sentiment analysis that assigns weights to words based on their frequency in a document relative to their frequency in the entire corpus. It helps capture the importance of words in a document relative to their significance in the entire dataset.\n",
    "\n",
    "## **`Justification`**\n",
    "\n",
    "**Interpretability** \n",
    "- `Naive Baye` and `SVM` are interpretable models as they give insights into which features contribute to the prediction, making it easier to understand the decision-making process.\n",
    "\n",
    "**Efficiency**\n",
    "- `Naive Bayes` is computationally efficient and can handle large datasets well.\n",
    "- `SVM` is efficient for high-dimensional data allows it to work effectively in non-linear scenarios.\n",
    "\n",
    "**Ensemble Learning**\n",
    "- `Random Forest` is an ensemble method that reduces overfitting and increases accuracy. It combines the outputs of multiple decision trees to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34242172",
   "metadata": {},
   "source": [
    "# Sentiment over a time period\n",
    "\n",
    "To analyze changes in sentiment over a specific time period, the column `date`, `preprocessed_text` and `sentiment` data are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a342c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e0d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d074646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833cc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18675f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b821b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%bg yellow\n",
    "\"*TO CONTINUE!!!*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bf5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e69d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308fc44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d59cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be43192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcdba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f75c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f184c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827112b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de24970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2e5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
